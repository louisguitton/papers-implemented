{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "from trax import models as tm\n",
    "from trax.supervised import training\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stream_fn = trax.data.TFDS(\n",
    "    'cnn_dailymail',\n",
    "     data_dir='data/',\n",
    "     keys=('article', 'highlights'),\n",
    "     train=True\n",
    ")\n",
    "\n",
    "eval_stream_fn = trax.data.TFDS(\n",
    "    'cnn_dailymail',\n",
    "    data_dir='data/',\n",
    "    keys=('article', 'highlights'),\n",
    "    train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://trax-ml/vocabs/en_32k.subword vocab_dir/en_32k.subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 2 vocab_dir/en_32k.subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "SEP = 0 # Padding or separator token\n",
    "EOS = 1 # End of sentence token\n",
    "\n",
    "# Concatenate tokenized inputs and targets using 0 as separator.\n",
    "def preprocess(stream):\n",
    "    for (article, summary) in stream:\n",
    "        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
    "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) # Accounting for EOS and SEP\n",
    "        yield joint, joint, np.array(mask)\n",
    "\n",
    "data_pipeline = trax.data.Serial(\n",
    "    trax.data.Tokenize(\n",
    "        vocab_file='en_32k.subword',\n",
    "        vocab_dir='vocab_dir'\n",
    "    ),\n",
    "    preprocess,\n",
    "    trax.data.Shuffle(),\n",
    "    trax.data.FilterByLength(max_length=2048),\n",
    "    trax.data.BucketByLength(\n",
    "        boundaries=[  32, 128, 512, 2048],\n",
    "        batch_sizes=[512, 128,  32,    8, 1],\n",
    "    ),\n",
    "    trax.data.AddLossWeights()\n",
    ")\n",
    "train_batches_stream = data_pipeline(train_stream_fn())\n",
    "eval_batches_stream = data_pipeline(eval_stream_fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(integers):\n",
    "    \"\"\"List of ints to str\"\"\"\n",
    "  \n",
    "    s = trax.data.detokenize(\n",
    "        integers,\n",
    "        vocab_file='en_32k.subword',\n",
    "        vocab_dir='vocab_dir'\n",
    "    )\n",
    "    \n",
    "    return wrapper.fill(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tm.TransformerLM(\n",
    "    vocab_size=33300,\n",
    "    d_model=4,\n",
    "    d_ff=16,\n",
    "    n_layers=1,\n",
    "    n_heads=2,\n",
    "    mode='train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task = training.TrainTask( \n",
    "  labeled_data=train_batches_stream,\n",
    "  loss_layer=tl.CrossEntropyLoss(),\n",
    "  optimizer=trax.optimizers.Adam(0.01),\n",
    "  lr_schedule=trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01),\n",
    "  n_steps_per_checkpoint=10\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask( \n",
    "  labeled_data=eval_batches_stream,\n",
    "  metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]\n",
    ")\n",
    "\n",
    "# Training loop saves checkpoints to output_dir.\n",
    "output_dir = os.path.expanduser('~/output-dir/')\n",
    "!rm -rf {output_dir}\n",
    "training_loop = training.Loop(model,\n",
    "                              train_task,\n",
    "                              eval_tasks=[eval_task],\n",
    "                              output_dir=output_dir)\n",
    "\n",
    "# Run 10 steps (batches).\n",
    "training_loop.run(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   `Realistic (pretrained) model: `                                 \n",
    "                                       \n",
    "    TransformerLM(vocab_size=33300, d_model=512, d_ff=2048, n_layers=6, n_heads=8, \n",
    "                   dropout=0.1, max_len=4096, ff_activation=tl.Relu)\n",
    "                   \n",
    "   `This model:`\n",
    "   \n",
    "    TransformerLM(d_model=4, d_ff=16, n_layers=1, n_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the model architecture\n",
    "model =  tm.TransformerLM(\n",
    "    vocab_size=33300,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    "    mode='predict'\n",
    ")\n",
    "\n",
    "# Load the pre-trained weights\n",
    "model.init_from_file(\"pretrained_weights.pkl.gz\", weights_only=True)\n",
    "\n",
    "# Tokenize a sentence.\n",
    "article = \"It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the 'Tebowing' craze at the school was blocking the hallway and presenting a safety hazard to students.\"\n",
    "tokenized = list(trax.data.tokenize(iter([article]),  # Operates on streams.\n",
    "                                    vocab_file='summarize32k.subword',\n",
    "                             vocab_dir='vocab_dir'))[0]\n",
    "\n",
    "# Decode from the Transformer.\n",
    "tokenized = tokenized[None, :]\n",
    "tokenized_summary = trax.supervised.decoding.autoregressive_sample(\n",
    "    model, tokenized, temperature=0.0)\n",
    "\n",
    "# De-tokenize,\n",
    "tokenized_summary = tokenized_summary[0][:-1]  # Remove batch and EOS.\n",
    "summary = trax.data.detokenize(tokenized_summary,\n",
    "                                   vocab_file='summarize32k.subword',\n",
    "                             vocab_dir='vocab_dir')\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_symbol(cur_output_tokens, model):\n",
    "    \"\"\"Returns the next symbol for a given sentence.\n",
    "\n",
    "    Args:\n",
    "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\n",
    "        model (trax.layers.combinators.Serial): The transformer model.\n",
    "\n",
    "    Returns:\n",
    "        int: tokenized symbol.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # current output tokens length\n",
    "    token_length = len(cur_output_tokens)\n",
    "    # calculate the minimum power of 2 big enough to store token_length\n",
    "    # HINT: use np.ceil() and np.log2()\n",
    "    # add 1 to token_length so np.log2() doesn't receive 0 when token_length is 0\n",
    "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n",
    "\n",
    "    # Fill cur_output_tokens with 0's until it reaches padded_length\n",
    "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
    "    padded_with_batch = np.array(padded)[None, :] # Don't replace this 'None'! This is a way of setting the batch dim\n",
    "\n",
    "    # model expects a tuple containing two padded tensors (with batch)\n",
    "    output, _ = model((padded_with_batch, padded_with_batch)) \n",
    "    # HINT: output has shape (1, padded_length, vocab_size)\n",
    "    # To get log_probs you need to index output with 0 in the first dim\n",
    "    # token_length in the second dim and all of the entries for the last dim.\n",
    "    log_probs = output[0, token_length, :]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return int(np.argmax(log_probs))\n",
    "\n",
    "def tokenize(input_str, EOS=1):\n",
    "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "  \n",
    "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
    "    # we get around it by making a 1-element stream with `iter`.\n",
    "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
    "                                      vocab_dir='vocab_dir/',\n",
    "                                      vocab_file='summarize32k.subword'))\n",
    "    \n",
    "    # Mark the end of the sentence with EOS\n",
    "    return list(inputs) + [EOS]\n",
    "\n",
    "def detokenize(integers):\n",
    "    \"\"\"List of ints to str\"\"\"\n",
    "  \n",
    "    s = trax.data.detokenize(integers,\n",
    "                             vocab_dir='vocab_dir/',\n",
    "                             vocab_file='summarize32k.subword')\n",
    "    \n",
    "    return wrapper.fill(s)\n",
    "\n",
    "# UNQ_C10\n",
    "# Decoding functions.\n",
    "def greedy_decode(input_sentence, model):\n",
    "    \"\"\"Greedy decode function.\n",
    "\n",
    "    Args:\n",
    "        input_sentence (string): a sentence or article.\n",
    "        model (trax.layers.combinators.Serial): Transformer model.\n",
    "\n",
    "    Returns:\n",
    "        string: summary of the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # Use tokenize()\n",
    "    cur_output_tokens = tokenize(input_sentence) + [0]\n",
    "    generated_output = [] \n",
    "    cur_output = 0 \n",
    "    EOS = 1 \n",
    "    \n",
    "    while cur_output != EOS:\n",
    "        # Get next symbol\n",
    "        cur_output = next_symbol(cur_output_tokens, model)\n",
    "        # Append next symbol to original sentence\n",
    "        cur_output_tokens.append(cur_output)\n",
    "        # Append next symbol to generated sentence\n",
    "        generated_output.append(cur_output)    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return detokenize(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It’s the posing craze sweeping the U.S. after being brought to fame by\n",
      "skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert\n",
      "Pujols - and even Republican politician Rick Perry. But now four\n",
      "students at Riverhead High School on Long Island, New York, have been\n",
      "suspended for dropping to a knee and taking up a prayer pose to mimic\n",
      "Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel,\n",
      "Tyler Carroll and Connor Carroll were all suspended for one day\n",
      "because the ‘Tebowing’ craze was blocking the hallway and presenting a\n",
      "safety hazard to students. Scroll down for video. Banned: Jordan\n",
      "Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured\n",
      "left) were all suspended for one day by Riverhead High School on Long\n",
      "Island, New York, for their tribute to Broncos quarterback Tim Tebow.\n",
      "Issue: Four of the pupils were suspended for one day because they\n",
      "allegedly did not heed to warnings that the 'Tebowing' craze at the\n",
      "school was blocking the hallway and presenting a safety hazard to\n",
      "students. \n",
      "\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students.<EOS>\n"
     ]
    }
   ],
   "source": [
    "# Get the model architecture\n",
    "model =  tm.TransformerLM(\n",
    "    vocab_size=33300,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    "    mode='eval'\n",
    ")\n",
    "\n",
    "# Load the pre-trained weights\n",
    "model.init_from_file(\"pretrained_weights.pkl.gz\", weights_only=True)\n",
    "\n",
    "# Test it out with a whole article!\n",
    "article = \"It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the 'Tebowing' craze at the school was blocking the hallway and presenting a safety hazard to students.\"\n",
    "print(wrapper.fill(article), '\\n')\n",
    "print(greedy_decode(article, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦁 Premier League Player of the Week - The never-ending story\n",
      "Presumably the film would have told the inspiring, feel-good story of\n",
      "Vardy being plucked from the obscurity of non-league football before\n",
      "going on to lift the Premier League title with Leicester in fairytale\n",
      "fashion.  Even at the ripe old age of 33 he is still one of Europe’s\n",
      "deadliest strikers, as he showed once again in Leicester’s sensational\n",
      "5-2 victory away at Manchester City on Sunday.  Vardy is the most\n",
      "dangerous player around when it comes to hanging off the shoulder of\n",
      "the last man and if you play with a high line against the Foxes,\n",
      "you’re asking for trouble.  Pep Guardiola took that risk in this game,\n",
      "and he would have paid the price much sooner had some of Leicester’s\n",
      "final balls been better on the counter-attack.  Vardy eventually did\n",
      "make his first big mark on the match when he ghosted past Kyle Walker\n",
      "and left the defender with little choice but to bring him down for a\n",
      "penalty. Up stepped the 2019/20 Golden Boot winner to fire past\n",
      "Ederson for his third goal of the new season.  Then in the second half\n",
      "came one of the most brilliant moments of Vardy’s career, when Timothy\n",
      "Castagne was played in down the right and his low cross into the box\n",
      "found the striker lurking.  In his post-match interview, Vardy\n",
      "described what he did next as a “Johan Dink”, and who are we to argue?\n",
      "To put the icing on the cake, he soon completed his hat-trick with\n",
      "another penalty, taking his tally to five goals for the season already\n",
      "– the most in the Premier League.  He has now scored 108 goals in 214\n",
      "Premier League matches. Not a bad haul for a player who didn’t make\n",
      "his debut in the top flight until he was 27.  He has also netted 21\n",
      "penalties since his Premier League debut – more than any other player\n",
      "during that period – and he’s won 18 of those spot-kicks himself.  And\n",
      "what’s more, he has now scored nine times against Guardiola’s\n",
      "Manchester City – more than any other player since the Catalan’s\n",
      "arrival in England. \n",
      "\n",
      " Ciy back scored nine times against Guardiola's Manchester City. The\n",
      "10-year-old also scored 108 goals in 214 Premier League matches. He\n",
      "has scored 108 goals in 214 Premier League matches.<EOS>\n"
     ]
    }
   ],
   "source": [
    "article = \"\"\"🦁 Premier League Player of the Week - The never-ending story\n",
    "\n",
    "Presumably the film would have told the inspiring, feel-good story of Vardy being plucked from the obscurity of non-league football before going on to lift the Premier League title with Leicester in fairytale fashion.\n",
    " Even at the ripe old age of 33 he is still one of Europe’s deadliest strikers, as he showed once again in Leicester’s sensational 5-2 victory away at Manchester City on Sunday.\n",
    " Vardy is the most dangerous player around when it comes to hanging off the shoulder of the last man and if you play with a high line against the Foxes, you’re asking for trouble.\n",
    " Pep Guardiola took that risk in this game, and he would have paid the price much sooner had some of Leicester’s final balls been better on the counter-attack.\n",
    " Vardy eventually did make his first big mark on the match when he ghosted past Kyle Walker and left the defender with little choice but to bring him down for a penalty. Up stepped the 2019/20 Golden Boot winner to fire past Ederson for his third goal of the new season.\n",
    " Then in the second half came one of the most brilliant moments of Vardy’s career, when Timothy Castagne was played in down the right and his low cross into the box found the striker lurking.\n",
    " In his post-match interview, Vardy described what he did next as a “Johan Dink”, and who are we to argue?\n",
    " To put the icing on the cake, he soon completed his hat-trick with another penalty, taking his tally to five goals for the season already – the most in the Premier League.\n",
    " He has now scored 108 goals in 214 Premier League matches. Not a bad haul for a player who didn’t make his debut in the top flight until he was 27.\n",
    " He has also netted 21 penalties since his Premier League debut – more than any other player during that period – and he’s won 18 of those spot-kicks himself.\n",
    " And what’s more, he has now scored nine times against Guardiola’s Manchester City – more than any other player since the Catalan’s arrival in England.\"\"\"\n",
    "\n",
    "print(wrapper.fill(article), '\\n')\n",
    "print(greedy_decode(article, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

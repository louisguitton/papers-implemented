{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset cnn_dailymail/plain_text/3.0.0 (download: 558.32 MiB, generated: 1.27 GiB, total: 1.82 GiB) to data/cnn_dailymail/plain_text/3.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d38d991b0794340a0f3784535450c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', layout=Layout(width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6074f9bd3548e982b2f4c9a9e9947b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', layout=Layout(width='20px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fd37c00c1e42e294266bf8acf7a2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aff22151ac9430b9c79b461d548e52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_stream_fn = trax.data.TFDS(\n",
    "    'cnn_dailymail',\n",
    "     data_dir='data/',\n",
    "     keys=('article', 'highlights'),\n",
    "     train=True\n",
    ")\n",
    "\n",
    "eval_stream_fn = trax.data.TFDS(\n",
    "    'cnn_dailymail',\n",
    "    data_dir='data/',\n",
    "    keys=('article', 'highlights'),\n",
    "    train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "SEP = 0 # Padding or separator token\n",
    "EOS = 1 # End of sentence token\n",
    "\n",
    "# Concatenate tokenized inputs and targets using 0 as separator.\n",
    "def preprocess(stream):\n",
    "    for (article, summary) in stream:\n",
    "        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
    "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) # Accounting for EOS and SEP\n",
    "        yield joint, joint, np.array(mask)\n",
    "\n",
    "data_pipeline = data.Serial(\n",
    "    data.Tokenize(vocab_file='summarize32k.subword.subwords'),\n",
    "    preprocess,\n",
    "    data.Shuffle(),\n",
    "    data.FilterByLength(max_length=2048),\n",
    "    data.BucketByLength(\n",
    "        boundaries=[  32, 128, 512, 2048],\n",
    "        batch_sizes=[512, 128,  32,    8, 1],\n",
    "    ),\n",
    "    data.AddLossWeights()\n",
    ")\n",
    "train_batches_stream = data_pipeline(train_stream_fn())\n",
    "eval_batches_stream = data_pipeline(eval_stream_fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax import models as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tm.TransformerLM(\n",
    "    d_model=4,\n",
    "    d_ff=16,\n",
    "    n_layers=1,\n",
    "    n_heads=2,\n",
    "    mode='train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task = training.TrainTask( \n",
    "  labeled_data=train_batches_stream,\n",
    "  loss_layer=tl.CrossEntropyLoss(),\n",
    "  optimizer=trax.optimizers.Adam(0.01),\n",
    "  lr_schedule=trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01),\n",
    "  n_steps_per_checkpoint=10\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask( \n",
    "  labeled_data=eval_batches_stream,\n",
    "  metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]\n",
    ")\n",
    "\n",
    "# Training loop saves checkpoints to output_dir.\n",
    "output_dir = os.path.expanduser('~/output-dir/')\n",
    "!rm -rf {output_dir}\n",
    "training_loop = training.Loop(model,\n",
    "                              train_task,\n",
    "                              eval_tasks=[eval_task],\n",
    "                              output_dir=output_dir)\n",
    "\n",
    "# Run 10 steps (batches).\n",
    "training_loop.run(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model architecture\n",
    "model = TransformerLM(mode='eval')\n",
    "\n",
    "# Load the pre-trained weights\n",
    "model.init_from_file('model.pkl.gz', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence.\n",
    "article = \"It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the 'Tebowing' craze at the school was blocking the hallway and presenting a safety hazard to students.\"\n",
    "tokenized = list(trax.data.tokenize(iter([article]),  # Operates on streams.\n",
    "                                    vocab_dir='gs://trax-ml/vocabs/',\n",
    "                                    vocab_file='ende_32k.subword'))[0]\n",
    "\n",
    "# Decode from the Transformer.\n",
    "tokenized = tokenized[None, :]  # Add batch dimension.\n",
    "tokenized_translation = trax.supervised.decoding.autoregressive_sample(\n",
    "    model, tokenized, temperature=0.0)  # Higher temperature: more diverse results.\n",
    "\n",
    "# De-tokenize,\n",
    "tokenized_translation = tokenized_translation[0][:-1]  # Remove batch and EOS.\n",
    "translation = trax.data.detokenize(tokenized_translation,\n",
    "                                   vocab_dir='gs://trax-ml/vocabs/',\n",
    "                                   vocab_file='ende_32k.subword')\n",
    "print(translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

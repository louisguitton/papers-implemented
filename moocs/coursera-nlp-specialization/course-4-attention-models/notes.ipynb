{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Neural Machine Translation\n",
    "\n",
    "- Seq2Seq suffers for long sentences because it has a fixed-termed memory length. Attention was brought up to get around that issue. (=it gives more weight to important words)\n",
    "- Alignment is used for word sense disambiguation\n",
    "- metaphor for attention's key, query, value triplet: \"Mom, I'm looking for my keys.\" then Mom thinks about the usual places your keys are and tells you what it the most likely place to find your keys. Attention is like a process to save time by looking in the place where the key is the most likely to be. \n",
    "- Keys and values are pairs, both coming from the encoder hidden state, whiles queries come from the decoder hidden states.\n",
    "\n",
    "- Teacher forcing provides faster training and higher accuracy by allowing the model to use the decoderâ€™s actual output to compare its predictions against\n",
    "- Evaluate machine translation models with BLEU and ROUGE scores\n",
    "- to sample and decode, there are multiple methods like :\n",
    "    - beam search which uses conditional probabilities and the beam width parameter\n",
    "    - MBR (minimum Bayes risk) takes several samples and compare them against each other to find the golden one\n",
    "    - greedy decoding \n",
    "    - random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Text summarization with Transformers\n",
    "\n",
    "- in English to German translation = encoder/decoder attention\n",
    "    - Keys = embeddings of english words\n",
    "    - Queries = embeddings of german words\n",
    "    - Values = the same as keys\n",
    "\n",
    "<img src=\"C4_W2_L3_dot-product-attention_S03_concept-of-attention_stripped.png\" width=400/>\n",
    "<img src=\"C4_W2_L3_dot-product-attention_S04_attention-math_stripped.png\" width=400/>\n",
    "\n",
    "softmax is used to transform the weights into a probability distribution\n",
    "\n",
    "- in text generation: causal attention or self-attention\n",
    "    - Keys = Queries = words from the same sentence\n",
    "    - Queries can look only in the past\n",
    "\n",
    "<img src=\"C4_W2_L4_causal-attention_S03_causal-attention-math_stripped.png\" width=400/>\n",
    "<img src=\"C4_W2_L4_causal-attention_S04_causal-attention-math-2_stripped.png\" width=400/>\n",
    "\n",
    "Transformer decoder = GPT2\n",
    "\n",
    "Can be used for summarization\n",
    "\n",
    "Summarization datasets :\n",
    "- 300k articles  https://www.tensorflow.org/datasets/catalog/cnn_dailymail\n",
    "\n",
    "- 45k articles https://www.tensorflow.org/datasets/catalog/multi_news\n",
    "\n",
    "- 1 M articles https://www.tensorflow.org/datasets/catalog/newsroom\n",
    "- what time it takes to train\n",
    "\n",
    "> This week you will use a language model -- Transformer Decoder -- to solve\n",
    "an input-output problem. As you know, language models only predict the next\n",
    "word, they have no notion of inputs. To create a single input suitable for\n",
    "a language model, we concatenate inputs with targets putting a separator\n",
    "in between. We also need to create a mask -- with 0s at inputs and 1s at targets -- so that the model is not penalized for mis-predicting the article and only focuses on the summary. See the preprocess function below for how this is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Question Answering \n",
    "\n",
    "- CBOW = fixed context window + Feed Forward Neural Network\n",
    "- ElMO = bi directional LSTM (subtype of RNN); improves on CBOW because it gets the context outside the fixed window\n",
    "- GPT = transformer with only Decoder (no Encoder) and uni-directional (because of causeal Attention: you can look only at the previous inputs)\n",
    "- BERT = transformer with only the Encoder (no Decoder) and bi-directional\n",
    "    - you can pre train it with multi-mask language modeling\n",
    "    - you can pre train it with next sentence prediction\n",
    "- T5 = transformer with Encoder and Decoder and bi-directional\n",
    "    - multi task with prefix\n",
    "\n",
    "\n",
    "Previously, for summarization, we've looked at the Decoder block. `trax.models.TransformerLM` uses such Decoder.\n",
    "\n",
    "Now, for question answering, we're looking at the Encoder block.\n",
    "\n",
    "You can use SentencePiece's implementation of the BPE algorithm to preprocess text data into subwords https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

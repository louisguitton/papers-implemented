{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [A Position-Biased PageRank Algorithm for Keyphrase Extraction](https://www.semanticscholar.org/paper/A-Position-Biased-PageRank-Algorithm-for-Keyphrase-Florescu-Caragea/407c61430a924fe4d260aa411523c1276f14f0e5)\n",
    "- https://github.com/DerwenAI/pytextrank/issues/78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import spacy\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Chelsea 'opted against' signing Salomon Rondón on deadline day.\n",
    "\n",
    "Chelsea reportedly opted against signing Salomón Rondón on deadline day despite their long search for a new centre forward.\n",
    "With Olivier Giroud expected to leave, the Blues targeted Edinson Cavani, Dries Mertens and Moussa Dembele – only to end up with none of them.\n",
    "According to Telegraph Sport, Dalian Yifang offered Rondón to Chelsea only for them to prefer keeping Giroud at the club.\n",
    "Manchester United were also linked with the Venezuela international before agreeing a deal for Shanghai Shenhua striker Odion Ighalo.\n",
    "Manager Frank Lampard made no secret of his transfer window frustration, hinting that to secure top four football he ‘needed’ signings.\n",
    "Their draw against Leicester on Saturday means they have won just four of the last 13 Premier League matches.\n",
    "\"\"\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "import math\n",
    "from typing import List, Tuple, Dict, Iterable, Set, Optional\n",
    "\n",
    "import networkx as nx\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Phrase:\n",
    "    text: str\n",
    "    rank: float\n",
    "    count: int\n",
    "    phrase_list: List[Span]\n",
    "\n",
    "\n",
    "Node = Tuple[str, str]  # (lemma, pos)\n",
    "\n",
    "\n",
    "class BaseTextRank:\n",
    "    _EDGE_WEIGHT = 1.0\n",
    "    _POS_KEPT = [\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n",
    "    _TOKEN_LOOKBACK = 3\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        edge_weight=_EDGE_WEIGHT,\n",
    "        pos_kept=_POS_KEPT,\n",
    "        scrubber=str.strip,\n",
    "        token_lookback=_TOKEN_LOOKBACK,\n",
    "    ):\n",
    "        self.edge_weight = edge_weight\n",
    "        self.pos_kept = pos_kept\n",
    "        self.scrubber = scrubber\n",
    "        self.token_lookback = token_lookback\n",
    "\n",
    "        self.doc = None\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        \"\"\"Call the inner EntityRuler.\"\"\"\n",
    "        self.doc = doc\n",
    "        Doc.set_extension(\"phrases\", force=True, default=[])\n",
    "        Doc.set_extension(\"textrank\", force=True, default=self)\n",
    "        doc._.phrases = self.calc_textrank()\n",
    "        return doc\n",
    "\n",
    "    def calc_textrank(self) -> List[Phrase]:\n",
    "        lemma_graph = self.construct_graph()\n",
    "\n",
    "        pagerank_personalization = self.get_personalization()\n",
    "\n",
    "        ranks: Dict[str, float] = nx.pagerank(\n",
    "            lemma_graph, personalization=pagerank_personalization\n",
    "        )\n",
    "\n",
    "        nc_phrases = self.collect_phrases(self.doc.noun_chunks, ranks)\n",
    "        ent_phrases = self.collect_phrases(self.doc.ents, ranks)\n",
    "        all_phrases = {**nc_phrases, **ent_phrases}\n",
    "\n",
    "        phrase_list: List[Phrase] = self.get_min_phrases(all_phrases)\n",
    "\n",
    "        return sorted(phrase_list, key=lambda p: p.rank, reverse=True)\n",
    "\n",
    "    def construct_graph(self) -> nx.Graph:\n",
    "        G = nx.Graph()\n",
    "        # add nodes made of (lemma, pos)\n",
    "        G.add_nodes_from(self.node_list)\n",
    "        # add edges between nodes co-occuring within a window, weighted by the count\n",
    "        G.add_edges_from(self.edge_list)\n",
    "        return G\n",
    "\n",
    "    @property\n",
    "    def node_list(self) -> List[Tuple[str, str]]:\n",
    "        nodes = [\n",
    "            (token.lemma_, token.pos_)\n",
    "            for token in self.doc\n",
    "            if token.pos_ in self.pos_kept\n",
    "        ]\n",
    "        return nodes\n",
    "\n",
    "    @property\n",
    "    def edge_list(self) -> List[Tuple[Node, Node, Dict[str, float]]]:\n",
    "        edges = []\n",
    "        for sent in self.doc.sents:\n",
    "            H = [\n",
    "                (token.lemma_, token.pos_)\n",
    "                for token in sent\n",
    "                if token.pos_ in self.pos_kept\n",
    "            ]\n",
    "            for hop in range(self.token_lookback):\n",
    "                for idx, node in enumerate(H[: -1 - hop]):\n",
    "                    nbor = H[hop + idx + 1]\n",
    "                    edges.append((node, nbor))\n",
    "\n",
    "        # Include weight on the edge: (2, 3, {'weight': 3.1415})\n",
    "        edges = [\n",
    "            (*n, dict(weight=w * self.edge_weight)) for n, w in Counter(edges).items()\n",
    "        ]\n",
    "        return edges\n",
    "\n",
    "    def get_personalization(self) -> Optional[Dict[Node, float]]:\n",
    "        return None\n",
    "\n",
    "    def collect_phrases(\n",
    "        self, spans: Iterable[Span], ranks: Dict[str, float]\n",
    "    ) -> Dict[Span, float]:\n",
    "        phrases = {\n",
    "            span: sum(\n",
    "                ranks[(token.lemma_, token.pos_)]\n",
    "                for token in span\n",
    "                if token.pos_ in self.pos_kept\n",
    "            )\n",
    "            for span in spans\n",
    "        }\n",
    "        return {\n",
    "            span: self._calc_discounted_normalised_rank(span, sum_rank)\n",
    "            for span, sum_rank in phrases.items()\n",
    "        }\n",
    "\n",
    "    def _calc_discounted_normalised_rank(self, span: Span, sum_rank: float) -> float:\n",
    "        non_lemma = len([tok for tok in span if tok.pos_ not in self.pos_kept])\n",
    "\n",
    "        # although the noun chunking is greedy, we discount the ranks using a\n",
    "        # point estimate based on the number of non-lemma tokens within a phrase\n",
    "        non_lemma_discount = len(span) / (len(span) + (2.0 * non_lemma) + 1.0)\n",
    "\n",
    "        # use root mean square (RMS) to normalize the contributions of all the tokens\n",
    "        phrase_rank = math.sqrt(sum_rank / (len(span) + non_lemma))\n",
    "\n",
    "        return phrase_rank * non_lemma_discount\n",
    "\n",
    "    def get_min_phrases(self, all_phrases=Dict[Span, float]) -> List[Phrase]:\n",
    "        data = [\n",
    "            (self.scrubber(span.text), rank, span) for span, rank in all_phrases.items()\n",
    "        ]\n",
    "\n",
    "        keyfunc = lambda x: x[0]\n",
    "        applyfunc = lambda g: list((rank, spans) for text, rank, spans in g)\n",
    "        phrases: List[Tuple[str, List[Tuple[float, Span]]]] = groupby_apply(\n",
    "            data, keyfunc, applyfunc\n",
    "        )\n",
    "\n",
    "        phrase_list = [\n",
    "            Phrase(\n",
    "                text=p[0],\n",
    "                rank=max(rank for rank, span in p[1]),\n",
    "                count=len(p[1]),\n",
    "                phrase_list=list(span for rank, span in p[1]),\n",
    "            )\n",
    "            for p in phrases\n",
    "        ]\n",
    "        return phrase_list\n",
    "\n",
    "\n",
    "def groupby_apply(data, keyfunc, applyfunc):\n",
    "    \"\"\"Groupby a key and sum without pandas dependency.\n",
    "\n",
    "    Arguments:\n",
    "        data: iterable\n",
    "        keyfunc: callable to define the key by which you want to group\n",
    "        applyfunc: callable to apply to the group\n",
    "\n",
    "    Returns:\n",
    "        Iterable with accumulated values.\n",
    "\n",
    "    Ref: https://docs.python.org/3/library/itertools.html#itertools.groupby\n",
    "    \"\"\"\n",
    "    data = sorted(data, key=keyfunc)\n",
    "    return [(k, applyfunc(g)) for k, g in itertools.groupby(data, keyfunc)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "\n",
    "\n",
    "\n",
    "class PositionRank(BaseTextRank):\n",
    "    def get_personalization(self) -> Optional[Dict[Node, float]]:\n",
    "        \"\"\"\n",
    "        Specifically, we propose to assign a higher probability to a word\n",
    "        found on the 2nd position as compared with a word found on the\n",
    "        50th position in the same document. The weight of each candidate\n",
    "        word is equal to its inverse position in the document.\n",
    "        If the same word appears multiple times in the target document,\n",
    "        then we sum all its position weights.\n",
    "        For example, a word v_i occurring in the following positions:\n",
    "        2nd, 5th and 10th, has a weight p(v_i) = 1/2 + 1/5 + 1/10 = 4/5 = 0.8\n",
    "        The weights of words are normalized before they are used in the\n",
    "        position-biased PageRank.\n",
    "        \"\"\"\n",
    "        weighted_tokens: List[Tuple[str, float]] = [\n",
    "            (tok, 1 / (i + 1))\n",
    "            for i, tok in enumerate(\n",
    "                token.lemma_ for token in self.doc if token.pos_ in self.pos_kept\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        keyfunc = lambda x: x[0]\n",
    "        applyfunc = lambda g: sum(w for text, w in g)\n",
    "        accumulated_weighted_tokens: List[Tuple[str, float]] = groupby_apply(\n",
    "            weighted_tokens, keyfunc, applyfunc\n",
    "        )\n",
    "        accumulated_weighted_tokens = sorted(\n",
    "            accumulated_weighted_tokens, key=lambda x: x[1]\n",
    "        )\n",
    "\n",
    "        norm_weighted_tokens = {\n",
    "            k: w / sum(w_ for _, w_ in accumulated_weighted_tokens)\n",
    "            for k, w in accumulated_weighted_tokens\n",
    "        }\n",
    "\n",
    "        weighted_nodes = {\n",
    "            (token.lemma_, token.pos_): norm_weighted_tokens[token.lemma_]\n",
    "            for token in self.doc\n",
    "            if token.pos_ in self.pos_kept\n",
    "        }\n",
    "        return weighted_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Phrase(text='Salomon Rondón', rank=0.1701999924977688, count=2, phrase_list=[Salomon Rondón, Salomon Rondón]),\n",
       " Phrase(text='Salomón Rondón', rank=0.16702503586296494, count=2, phrase_list=[Salomón Rondón, Salomón Rondón]),\n",
       " Phrase(text='Chelsea', rank=0.15139173066873274, count=5, phrase_list=[\n",
       " Chelsea, Chelsea, Chelsea, Chelsea, Chelsea]),\n",
       " Phrase(text='deadline day', rank=0.15057992229816541, count=2, phrase_list=[deadline day, deadline day]),\n",
       " Phrase(text='Rondón', rank=0.15013300784484088, count=2, phrase_list=[Rondón, Rondón]),\n",
       " Phrase(text='Dalian Yifang', rank=0.0977676631859978, count=2, phrase_list=[Dalian Yifang, Dalian Yifang]),\n",
       " Phrase(text='Olivier Giroud', rank=0.08862796221453886, count=2, phrase_list=[Olivier Giroud, Olivier Giroud]),\n",
       " Phrase(text='Giroud', rank=0.08205056885278611, count=2, phrase_list=[Giroud, Giroud]),\n",
       " Phrase(text='Telegraph Sport', rank=0.07833387901020238, count=2, phrase_list=[Telegraph Sport, Telegraph Sport]),\n",
       " Phrase(text='Edinson Cavani', rank=0.06778463585836095, count=2, phrase_list=[Edinson Cavani, Edinson Cavani])]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_rank = PositionRank()\n",
    "processed_doc = position_rank(doc)\n",
    "processed_doc._.phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Phrase(text='Salomon Rondón', rank=0.09871585315384829, count=2, phrase_list=[Salomon Rondón, Salomon Rondón]),\n",
       " Phrase(text='Salomón Rondón', rank=0.09871585315384829, count=2, phrase_list=[Salomón Rondón, Salomón Rondón]),\n",
       " Phrase(text='Shanghai Shenhua striker Odion Ighalo', rank=0.09814252545640166, count=1, phrase_list=[Shanghai Shenhua striker Odion Ighalo]),\n",
       " Phrase(text='deadline day', rank=0.09102501388558137, count=2, phrase_list=[deadline day, deadline day]),\n",
       " Phrase(text='Rondón', rank=0.0893689802029807, count=2, phrase_list=[Rondón, Rondón]),\n",
       " Phrase(text='Shanghai Shenhua', rank=0.08608180019168989, count=1, phrase_list=[Shanghai Shenhua]),\n",
       " Phrase(text='Dries Mertens', rank=0.08353691638443825, count=2, phrase_list=[Dries Mertens, Dries Mertens]),\n",
       " Phrase(text='Edinson Cavani', rank=0.08231821728419356, count=2, phrase_list=[Edinson Cavani, Edinson Cavani]),\n",
       " Phrase(text='Moussa Dembele', rank=0.08208705978960228, count=2, phrase_list=[Moussa Dembele, Moussa Dembele]),\n",
       " Phrase(text='Manager Frank Lampard', rank=0.08161163836774048, count=1, phrase_list=[Manager Frank Lampard])]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_text_rank = BaseTextRank()\n",
    "comparison_doc = base_text_rank(doc)\n",
    "comparison_doc._.phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
